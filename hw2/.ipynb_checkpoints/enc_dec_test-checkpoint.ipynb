{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "Neural Machine Translation - Encoder Decoder model\n",
    "    Chainer implementation of an encoder-decoder sequence to sequence\n",
    "    model using bi-directional LSTM encoder\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chainer\n",
    "from chainer import cuda, Function, gradient_check, report, training, utils, Variable\n",
    "from chainer import datasets, iterators, optimizers, serializers\n",
    "from chainer import Link, Chain, ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.training import extensions\n",
    "from chainer.functions.array import concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Japanese English dataset configuration\n"
     ]
    }
   ],
   "source": [
    "# Import configuration file\n",
    "from nmt_config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(Chain):\n",
    "\n",
    "    '''\n",
    "    Constructor to initialize model\n",
    "    Params:\n",
    "        vsize_enc   - vocabulary size for source language (fed into encoder)\n",
    "        vsize_dec   - vocabulary size for target language (fed into decoder)\n",
    "        n_units     - size of the LSTMs\n",
    "        attn        - specifies whether to use attention\n",
    "    '''\n",
    "    def __init__(self, vsize_enc, vsize_dec,\n",
    "                 nlayers_enc, nlayers_dec,\n",
    "                 n_units, gpuid, attn=False):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        #--------------------------------------------------------------------\n",
    "        # add encoder layers\n",
    "        #--------------------------------------------------------------------\n",
    "        # add embedding layer\n",
    "        self.add_link(\"embed_enc\", L.EmbedID(vsize_enc, n_units))\n",
    "\n",
    "        '''\n",
    "        ___QUESTION-1-DESCRIBE-A-START___\n",
    "\n",
    "        - Explain the following lines of code\n",
    "        - Think about what add_link() does and how can we access Links added in Chainer.\n",
    "        - Why are there two loops or adding links?\n",
    "        '''\n",
    "        self.lstm_enc = [\"L{0:d}_enc\".format(i) for i in range(nlayers_enc)]\n",
    "        for lstm_name in self.lstm_enc:\n",
    "            self.add_link(lstm_name, L.LSTM(n_units, n_units))\n",
    "\n",
    "        self.lstm_rev_enc = [\"L{0:d}_rev_enc\".format(i) for i in range(nlayers_enc)]\n",
    "        for lstm_name in self.lstm_rev_enc:\n",
    "            self.add_link(lstm_name, L.LSTM(n_units, n_units))\n",
    "        '''\n",
    "        ___QUESTION-1-DESCRIBE-A-END___\n",
    "        '''\n",
    "\n",
    "        #--------------------------------------------------------------------\n",
    "        # add decoder layers\n",
    "        #--------------------------------------------------------------------\n",
    "        # add embedding layer\n",
    "        '''\n",
    "        ___QUESTION-1-DESCRIBE-B-START___\n",
    "        Comment on the input and output sizes of the following layers:\n",
    "        - L.EmbedID(vsize_dec, 2*n_units)\n",
    "        - L.LSTM(2*n_units, 2*n_units)\n",
    "        - L.Linear(2*n_units, vsize_dec)\n",
    "\n",
    "        Why are we using multipliers over the base number of units (n_units)?\n",
    "        '''\n",
    "\n",
    "        self.add_link(\"embed_dec\", L.EmbedID(vsize_dec, 2*n_units))\n",
    "\n",
    "        # add LSTM layers\n",
    "        self.lstm_dec = [\"L{0:d}_dec\".format(i) for i in range(nlayers_dec)]\n",
    "        for lstm_name in self.lstm_dec:\n",
    "            self.add_link(lstm_name, L.LSTM(2*n_units, 2*n_units))\n",
    "\n",
    "        if attn > 0:\n",
    "            # __QUESTION Add attention\n",
    "            pass\n",
    "\n",
    "        # Save the attention preference\n",
    "        # __QUESTION you should use this flag to check if attention\n",
    "        # has been selected. Your code should work with and without attention\n",
    "        self.attn = attn\n",
    "\n",
    "        # add output layer\n",
    "        self.add_link(\"out\", L.Linear(2*n_units, vsize_dec))\n",
    "        '''\n",
    "        ___QUESTION-1-DESCRIBE-B-END___\n",
    "        '''\n",
    "\n",
    "        # Store GPU id\n",
    "        self.gpuid = gpuid\n",
    "        self.n_units = n_units\n",
    "\n",
    "    def reset_state(self):\n",
    "        # reset the state of LSTM layers\n",
    "        for lstm_name in self.lstm_enc + self.lstm_rev_enc + self.lstm_dec:\n",
    "            self[lstm_name].reset_state()\n",
    "        self.loss = 0\n",
    "\n",
    "    '''\n",
    "        ___QUESTION-1-DESCRIBE-C-START___\n",
    "\n",
    "        Describe what the function set_decoder_state() is doing. What are c_state and h_state?\n",
    "    '''\n",
    "    # F- chainer.function\n",
    "    def set_decoder_state(self):\n",
    "        xp = cuda.cupy if self.gpuid >= 0 else np\n",
    "        c_state = F.concat((self[self.lstm_enc[-1]].c, self[self.lstm_rev_enc[-1]].c))\n",
    "        h_state = F.concat((self[self.lstm_enc[-1]].h, self[self.lstm_rev_enc[-1]].h))\n",
    "        self[self.lstm_dec[0]].set_state(c_state, h_state)\n",
    "\n",
    "    '''___QUESTION-1-DESCRIBE-C-END___'''\n",
    "\n",
    "    '''\n",
    "    Function to feed an input word through the embedding and lstm layers\n",
    "        args:\n",
    "        embed_layer: embeddings layer to use\n",
    "        lstm_layer:  list of names of lstm layers to use\n",
    "    '''\n",
    "    def feed_lstm(self, word, embed_layer, lstm_layer_list, train):\n",
    "        # get embedding for word\n",
    "        embed_id = embed_layer(word)\n",
    "        # feed into first LSTM layer\n",
    "        hs = self[lstm_layer_list[0]](embed_id)\n",
    "        # feed into remaining LSTM layers\n",
    "        for lstm_layer in lstm_layer_list[1:]:\n",
    "            hs = self[lstm_layer](hs)\n",
    "\n",
    "    # Function to encode an source sentence word\n",
    "    def encode(self, word, lstm_layer_list, train):\n",
    "        self.feed_lstm(word, self.embed_enc, lstm_layer_list, train)\n",
    "\n",
    "    # Function to decode a target sentence word\n",
    "    def decode(self, word, train):\n",
    "        self.feed_lstm(word, self.embed_dec, self.lstm_dec, train)\n",
    "\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    def encode_list(self, in_word_list, train=True):\n",
    "        xp = cuda.cupy if self.gpuid >= 0 else np\n",
    "        # convert list of tokens into chainer variable list\n",
    "        var_en = (Variable(xp.asarray(in_word_list, dtype=np.int32).reshape((-1,1)),\n",
    "                           volatile=(not train)))\n",
    "\n",
    "        var_rev_en = (Variable(xp.asarray(in_word_list[::-1], dtype=np.int32).reshape((-1,1)),\n",
    "                           volatile=(not train)))\n",
    "\n",
    "        # array to store hidden states for each word\n",
    "        # enc_states = xp.empty((0,2*self.n_units), dtype=xp.float32)\n",
    "        first_entry = True\n",
    "\n",
    "        # encode tokens\n",
    "        for f_word, r_word in zip(var_en, var_rev_en):\n",
    "            '''\n",
    "            ___QUESTION-1-DESCRIBE-D-START___\n",
    "\n",
    "            - Explain why we are performing two encode operations\n",
    "            '''\n",
    "            self.encode(f_word, self.lstm_enc, train)\n",
    "            self.encode(r_word, self.lstm_rev_enc, train)\n",
    "\n",
    "            '''___QUESTION-1-DESCRIBE-D-END___'''\n",
    "\n",
    "\n",
    "            # __QUESTION -- Following code is to assist with ATTENTION\n",
    "            # enc_states stores the hidden state vectors of the encoder\n",
    "            # this can be used for implementing attention\n",
    "            if first_entry == False:\n",
    "                h_state = F.concat((self[self.lstm_enc[-1]].h, self[self.lstm_rev_enc[-1]].h), axis=1)\n",
    "                enc_states = F.concat((enc_states, h_state), axis=0)\n",
    "            else:\n",
    "                enc_states = F.concat((self[self.lstm_enc[-1]].h, self[self.lstm_rev_enc[-1]].h), axis=1)\n",
    "                first_entry = False\n",
    "\n",
    "        return enc_states\n",
    "\n",
    "\n",
    "    # Select a word from a probability distribution\n",
    "    # should return a chainer variable\n",
    "    def select_word(self, prob, train=True, sample=False):\n",
    "        xp = cuda.cupy if self.gpuid >= 0 else np\n",
    "        if not sample:\n",
    "            indx = xp.argmax(prob.data[0])\n",
    "            pred_word = Variable(xp.asarray([indx], dtype=np.int32), volatile=not train)\n",
    "        else:\n",
    "            '''\n",
    "            ___QUESTION-2-SAMPLE\n",
    "\n",
    "            - Add code to sample from the probability distribution to\n",
    "            choose the next word\n",
    "            '''\n",
    "            pass\n",
    "        return pred_word\n",
    "\n",
    "    def encode_decode_train(self, in_word_list, out_word_list, train=True, sample=False):\n",
    "        xp = cuda.cupy if self.gpuid >= 0 else np\n",
    "        self.reset_state()\n",
    "        # Add GO_ID, EOS_ID to decoder input\n",
    "        decoder_word_list = [GO_ID] + out_word_list + [EOS_ID]\n",
    "        # encode list of words/tokens\n",
    "        enc_states = self.encode_list(in_word_list, train=train)\n",
    "        # initialize decoder LSTM to final encoder state\n",
    "        self.set_decoder_state()\n",
    "        # decode and compute loss\n",
    "        # convert list of tokens into chainer variable list\n",
    "        var_dec = (Variable(xp.asarray(decoder_word_list, dtype=np.int32).reshape((-1,1)),\n",
    "                            volatile=not train))\n",
    "        # Initialise first decoded word to GOID\n",
    "        pred_word = Variable(xp.asarray([GO_ID], dtype=np.int32), volatile=not train)\n",
    "\n",
    "        # compute loss\n",
    "        self.loss = 0\n",
    "        # decode tokens\n",
    "        for next_word_var in var_dec[1:]:\n",
    "            self.decode(pred_word, train=train)\n",
    "            if self.attn == NO_ATTN:\n",
    "                predicted_out = self.out(self[self.lstm_dec[-1]].h)\n",
    "            else:\n",
    "                # __QUESTION Add attention\n",
    "                pass\n",
    "\n",
    "            # compute loss\n",
    "            prob = F.softmax(predicted_out)\n",
    "\n",
    "            pred_word = self.select_word(prob, train=train, sample=False)\n",
    "            # pred_word = Variable(xp.asarray([pred_word.data], dtype=np.int32), volatile=not train)\n",
    "            '''\n",
    "            ___QUESTION-1-DESCRIBE-E-START___\n",
    "            Explain what loss is computed with an example\n",
    "            What does this value mean?\n",
    "            '''\n",
    "            self.loss += F.softmax_cross_entropy(predicted_out, next_word_var)\n",
    "            '''___QUESTION-1-DESCRIBE-E-END___'''\n",
    "\n",
    "        report({\"loss\":self.loss},self)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def decoder_predict(self, start_word, enc_states, max_predict_len=MAX_PREDICT_LEN, sample=False):\n",
    "        xp = cuda.cupy if self.gpuid >= 0 else np\n",
    "\n",
    "        # __QUESTION -- Following code is to assist with ATTENTION\n",
    "        # alpha_arr should store the alphas for every predicted word\n",
    "        alpha_arr = xp.empty((0,enc_states.shape[0]), dtype=xp.float32)\n",
    "\n",
    "        # return list of predicted words\n",
    "        predicted_sent = []\n",
    "        # load start symbol\n",
    "        pred_word = Variable(xp.asarray([start_word], dtype=np.int32), volatile=True)\n",
    "        pred_count = 0\n",
    "\n",
    "        # start prediction loop\n",
    "        while pred_count < max_predict_len and (int(pred_word.data) != (EOS_ID)):\n",
    "            self.decode(pred_word, train=False)\n",
    "\n",
    "            if self.attn == NO_ATTN:\n",
    "                prob = F.softmax(self.out(self[self.lstm_dec[-1]].h))\n",
    "            else:\n",
    "                # __QUESTION Add attention\n",
    "                pass\n",
    "\n",
    "            pred_word = self.select_word(prob, train=False, sample=sample)\n",
    "            # add integer id of predicted word to output list\n",
    "            predicted_sent.append(int(pred_word.data))\n",
    "            pred_count += 1\n",
    "        # __QUESTION Add attention\n",
    "        # When implementing attention, make sure to use alpha_array to store\n",
    "        # your attention vectors.\n",
    "        # The visualisation function in nmt_translate.py assumes such an array as input.\n",
    "        return predicted_sent, alpha_arr\n",
    "\n",
    "    def encode_decode_predict(self, in_word_list, max_predict_len=20, sample=False):\n",
    "        xp = cuda.cupy if self.gpuid >= 0 else np\n",
    "        self.reset_state()\n",
    "        # encode list of words/tokens\n",
    "        in_word_list_no_padding = [w for w in in_word_list if w != PAD_ID]\n",
    "        enc_states = self.encode_list(in_word_list, train=False)\n",
    "        # initialize decoder LSTM to final encoder state\n",
    "        self.set_decoder_state()\n",
    "        # decode starting with GO_ID\n",
    "        predicted_sent, alpha_arr = self.decoder_predict(GO_ID, enc_states,\n",
    "                                                         max_predict_len, sample=sample)\n",
    "        return predicted_sent, alpha_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
